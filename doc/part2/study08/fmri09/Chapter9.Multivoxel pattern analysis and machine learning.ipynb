{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###9.1. 패턴 분류 (pattern classification) 소개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 지금까지 논의된 통계 기법: 가지고 있는 데이터 세트를 최선으로 설명하려고 노력한다는 공통 특징 (=>inference)\n",
    "\n",
    "예) 일반선형보형(general linear model): 관찰 데이터와 fitted 데이터 사이의 평균 자승화(mean squared difference)가 최소화 되는 모형 매개함수(model paramter)들을 결정하는 방법\n",
    "\n",
    "- 기계학습(machine learning), statistical learning, 패턴인식(pattern recognition): 새로운 관찰치에 대한 가장 정확한 예측(prediction)을 가능하게 하는 모형 매개변수를 찾는다. 이때 이러한 모형 매개변수들이 항상 동일하지는 않음.\n",
    "\n",
    "참고 교재) 기계학습 - Alpaydin (2004), Bishop (2006), Duda et al. (2001), and Hastie et al. (2001)\n",
    "fMRI 기계학습(multivoxel pattern analysis or MVPA) - Haynes & Rees (2006), Norman et al. (2006), and O’Toole et al. (2007)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고자료) Statistical Learning\n",
    "\n",
    "1. Course Site: https://statlearning.class.stanford.edu\n",
    "\n",
    "2. Video: https://www.youtube.com/watch?v=St2-97n7atk\n",
    "http://www.youtube.com/channel/UC4OWDcPB1peiBXDfCSZ3h-w/playlists\n",
    "\n",
    "3. Materials\n",
    "http://www-bcf.usc.edu/~gareth/ISL/\n",
    "(Book:\n",
    "http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Fourth%20Printing.pdf\n",
    "Data Sets:\n",
    "http://www-bcf.usc.edu/~gareth/ISL/data.html\n",
    "R code:\n",
    "http://www-bcf.usc.edu/~gareth/ISL/code.html\n",
    "https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2014/0d68641c19484ef9aa6aeea03426dc68/\n",
    "=> 위 링크와 동일한 코드 내용. markdown,html 포맷으로 정리되어 있으며 코스 등록한 사람만 볼 수 있음.\n",
    ")\n",
    "\n",
    "4. Other related slides and video tutorials\n",
    "http://www.alsharif.info/#!iom530/c21o7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####9.1.1 An overview of the machine learning approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 목적: 관찰된 데이터로부터 아직 관찰되지 않은 데이터에 대해 일반화하고 예측\n",
    "\n",
    "예1)둘 중 어느 약이 새로운 정신과 환자를 더 효과적으로 치료할지 예측\n",
    "예2)개인에서 나타난 정신 과정의 유형을 해독하기 위해 fMRI 자료를 사용\n",
    "\n",
    "- 분류(classification): 예측이 별개의 범주들 중 하나일 때 => fMRI에 많이 사용. outcome measurement Y : finite, unordered set (survived/died, digit 0-9, cancer class of tissue sample)\n",
    "- 회귀(regression): 연속적인 값일때. Y: quantitative (e.g price, blood pressure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######9.1.1.1 Features, observations, “curse of dimensionality”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 목표: observed features (intensity of voxels)와 outcome variable (mental state / psychiatric diagnosis) 간의 관계를 학습\n",
    "- GLM(generalized linear model) 모형의 부적합성: fMRI에서 feature 수(>=5,0000)가 observation 수(20~100)보다 훨씬 큼. GLM에서는 feature 수 < observation 수\n",
    "- 다 차원(dimension): 차원을 더함에 모든 표본의 수가 기하급수적으로 증가하게 됨. \"curse of dimensionality” (Bellman, 1961)\n",
    "- 해결책\n",
    "  1) 다차원에 거쳐 중복되는 자료의 기저에 있는 더 적은 대안전 차원의 세트를 찾아내기 위해 PCA(Principal Components Analysis)나 ICA(Independent Component Analysis)와 같은 차원 축소(dimensionality reduction) 기법 사용. [Example of scikit-learn](http://scikit-learn.org/stable/auto_examples/decomposition/plot_ica_vs_pca.html)\n",
    "  2) 신호가 자료 내에서 산재하고 있다고 가정하고, 적은 수의 특징을 찾는 데 집중. 희소해법(sparse solution)들을 찾도록 설계된 분류자를 사용하거나, 조사되고 있는 특징의 수를 축소시키는 특징 선택(feature selection) 기법 사용."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####9.1.1.2 Overfitting (과적합)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "많은 매개변수를 가진 복잡한 모형은 훈련(training)데이터에 더 정확하게 들어 맞지만, 덜 복잡한 함수 (선형 또는 이차 함수)에 복잡한 모형을 사용하면 훈련 데이터의 잡음에 의해 과적합(overfitting)으로 나타나며 검사(test)데이터에 대해 저조한 일반화를 보이게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig9-1.jpeg\" width=630 height=1600/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###9.2 Applying classifiers to fMRI data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classifier를 fMRI 데이터에 적용하는 과정 (multivoxel pattern analysis (MVPA))은 다음 단계를 따른다.\n",
    "- Data extraction: classifier를 train하고 test에 사용되는 데이터를 추출.\n",
    "- Feature selection: 어떤 feature가 분석에 포함 될 것인가 선택.\n",
    "- Training, testing : classifier를 데이터에 훈련시키고 out-of-sample(표본 이상)에 대한 일반화의 정확도를 결정.\n",
    "- Classifier characterization: 어떤 feature가 분류 정확도에 가장 중요한지 결정\n",
    "\n",
    "Reference) [Multivoxel Pattern Analysis for fMRI Data: A Review\n",
    " ](http://www.hindawi.com/journals/cmmm/2012/961257/)\n",
    " \n",
    "Program) [BrainVoyager QX v2.8](http://www.brainvoyager.com/bvqx/doc/UsersGuide/MVPA/BasicConcepts.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###9.3 Data extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fMRI 시계열로부터 추출되는 데이터는 분류의 속성에 의존함. 특정 개인에 대한 서로 다른 사건들의 분류(within-subject classification)에서 어려운 점은 각 사건에 의해 유발된 활동을 가장 잘 반영하는 특징을 추출하는 것.\n",
    "블록 설계에서는 블록 내 모든 시점을 포함하거나, 또는 그 블록에 대한 GLM 모형의 평균이나 배타 값과 같은 시점들의 요약된 정보를 사용 가능.\n",
    "상대적으로 긴(10초 이상) 시행 간격을 가지는 사건 관련 설계에서는 비교적 다른 사건이 거의 혼입되지 않은 각 사건에 대해 유발된 신호를 추출 가능 \n",
    "- beta-series correlation model(베타 시리즈 상관 모형): 혈류역학 반응의 최고점을 나타내는 4~6초 후의 단일 시점을 단순히 취한다. 느리거나 빠른 사건 관련 설계에 모두 적용 가능\n",
    "- Ridge regression: 빠른 설계에서 사건들 간의 시간적 인접성이 매우 가변적인 추정치를 야기하는 설계 행렬에서 상당한 상관으로 나타나는 문제를 해결하나 민감도는 낮음\n",
    "- Spatiotemporal classifier: 각 시행의 전체 시계열을 분류자에 넣는 것. 분류자가 어떤 시점이 관련된 정보를 포함하고 있는지 결정. 혈류역학 반응에 관한 어떤 특정 형태도 가정하지 않고 민감성의 장점. 특징 수가 증가되어 학습에 어려움\n",
    "- Trial-by-trial classification(시행마다 분류): classify between summary statistic maps estimated for separate portions of the data (자료의 분리된 부분들에 대해 추정된 요약 통계 지도들간 분류) => 시행마다 반응 추정에 관련된 문제를 제거하지만, 결과로부터 이끌어 낼 수 있는 결론을 약간 변화시킴. 개별 시행보다는 분류되고 있는 구분에 대해 얼마나 많은 정보가 제시되는가에 대해 보다 전반적인 측정치를 제공.\n",
    "- Classify across individuals: parameter estimate map(모수 추정치 지도), t/Z-statistic map 와 같이 각 개인에 대해서 공간적으로 표준화된 요약 지도의 한 형태를 사용. 복셀에 걸쳐 동일한 척도를 가지는 통계적 지도를 사용하는 것이 유용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###9.4 Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어떤 특징(복셀)을 분류자 분석에 포함시킬 것인가를 결정</br>\n",
    "\n",
    "2가지 계층\n",
    " 1) 결과 변수와 관련되지 않은 사전 해부학적 지식 또는 데이터의 특징을 이용함으로써 결과 변수에 관한 어떤 지식도 포함하지 않는 방식으로 복셀을 구분해냄\n",
    " 2) 어떤 복셀들이 분류에 가장 유용할지를 확인하기 위해 분류자를 사용한 다음, 그 특징들만을 분석에 사용."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Whole-brain approach: 전체 뇌에서 나온 데이터를 모두 포함. 복셀의 크기에 따라 3만개까지 특징이 포함되는데 일부 분류자의 한계(예:신경망)를 넘는다. Support vector machine이 일반적으로 이러한 종류의 분석에 수행을 잘함. 정보가 어디에서 발견되는가에 관계 없이 특정한 기능적 구분에 관련된 정보가 뇌 안에 있는지의 여부일 때 가장 적절\n",
    "2. Priori regions of interest(ROI): ROI 규정에 대한 전략은 independent functional localizers(독립적인 기능적 국지화), individualized anatomical labels(개인화된 해부학적 라벨), population-based anatomical atlases(집단 기반 해부학적 지도)의 사용을 포함. 특정 기능에 있어 해부학적 또는 기능적으로 정의된 특정 영역의 상대적 중요성을 확인하거나, 특정한 이전 관심 영역에 집중할때 특히 유용. ROI선택이 그 분석의 결과로부터 얻어진 어떠한 지식에도 근거하지 않는다는 것을 확인하는 것이 중요.=>결과로서 나오는 일반화 추정치를 편향시킬 수 있기 때문.\n",
    "3. Unsupervised feature selection: 해부학적 유도된 특징 선택을 넘어서, 결과 변수와 관련이 없는 개인적인 특징의 특성을 이용하여 사전적이지 않은 특징 선택을 수행. 예) 관찰치들에 걸쳐 가장 높은 변량을 가지는 특징 => 더 적은변량을 가진 특징들보다 관찰치들 간 차이에 대한 정보를 더 많이 가지고 있을 것이라는 가정을 할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###9.5 Training and testing the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 전처리 후, 그 다음 단계는 분류자를 훈련시키고, 분류자의 에측값들을 결과 변인의 알려진 실제 값들과 비교함으로써 그 분류자의 일반화 수행을 평가하는 것. 분리된 데이터를 사용하는 것이 매우 중요 (글상자 9-1).</br>\n",
    "1)두개 데이터 세트를 하나는 분류자를 훈련하고 다른 하나는 시험하는데 사용하는 것은 단순하지만 관찰치의 2배를 수집해야 하기 때문에 비효율적이고 결과가 변동적일 수 있음.</br>\n",
    "2) k-fold cross-validation(k-겹 교차 타당화): k개 관측 블록에서 분류자가 하나를 제외한 모든 블록에서 훈련되고 남은 하나의 블록에서 시험되는 과정을 반복. k가 관찰치의 수와 동일하다면 leave-one-out cross-validation(하나-제외 교차 타당화)라고 불린다. 이는 계산적 대가가 상대적으로 크지만, 시험의 정확성에 대한 비편향 측정치를 제공할 것. 큰 k 값을 가지는 교차 타당화는 편향될 수 있다. 보통 10-fold 많이 사용.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://chrisjmccormick.files.wordpress.com/2013/07/10_fold_cv.png\" width=630>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####9.5.1 Feature selection/elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross-validation(교차 타당화) 내에서 feature selection(특징 선택)을 사용하는 데 있어 신호가 없이 잡음을 더하는 noninformative features(정보가가 없는 특징)을 제외함으로써 특징들의 수를 줄이는 것이 일반적이다. \n",
    "recursive feature elimination(순환적 선택 제거)보다 정교한 접근 방법. 분류자가 반복적으로 적합되고, 각 단계에서 가장 정보가가 낮은 특정한 수 또는 비율의 특징들이 제거됨. 특징 선택이 시험 세트와 분리된 훈련 데이터에서 수행되도록 하는 것이 중요.\n",
    "dimensionality reduction(차원성 축소) 기법을 사용해도 특징들 간의 상관에 의해 큰 데이터 세트에 출현한 거의 모든 정보를 가져오게 됨. matrix decomposition method(PCA, ICA) 방법이 이러한 목적으로 사용될 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####9.5.2 Classifiers for fMRI data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "분류 분석을 수행하기 전에 기법을 고르는 것이 필수적이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######9.5.2.1 Linear vs. nonlinear classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Linear classifiers(선형 분류자): 각 유형의 효과와 연합된 데이터 포인트들을 선형함수를 사용하여 분리할 수 있는 효과들을 탐지할 수 있음. 이차원은 하나의 선, 고차원에서는 hyperplne(초평면)이 됨\n",
    "\n",
    "- Nonlinear classifiers(비선형 분류자): 복잡한 함수에 의해 정의되는 효과를 탐지할 수 있음.\n",
    "\n",
    "- fMRI 분류에 주로 선형 분류자를 사용하는 이유\n",
    " 1. 이해하기 쉬움\n",
    " 2. 일반적으로 특성화하기 쉬움\n",
    " 3. optimization(최적화)를 필요로 하는 부가적인 매개변수를 가지며, 이는 일반적으로 추가적인 수준에서 cross-validation(교차 타당화)를 필요로 한다.\n",
    " \n",
    "<img src=\"fig9-2.jpeg\" width=630/>\n",
    "\n",
    "<img src=\"fig9-3.jpeg\" width=630/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####9.5.2.2 Computational limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "support vector machine과 같은 classifier는 매우 큰 데이터 세트(수십만개의 특징)가 합리적인 시간간(1~2시간) 내에 적합될 수 있는 방식으로 시행.\n",
    "neural network는 5천~1만개의 특징을 가진 데이터 세트에서 사용 가능한데, 이 수준에서 메모리 요구가 제한 요소가 된다.\n",
    "선형 판별 분석은 특징의 수가 관측치 수보다 적어야 하므로, 이 한계를 해결하기 위해 차원성 축소와 결합된 형태로 사용됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####9.5.3 Which classifier is best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"no free lunch\" theorem in machine learning: 각각의 분류자는 특정한 가정에 잘 맞는 데이터 세트에서 잘 수행하는 반면 잘 맞지 않는 다른 데이터 세트에서는 저조한 수행을 보임.\n",
    "데이터에 행해지는 모형 선택은 최종 시험 데이터와 분리되어야 함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####9.5.4 Assessing classifier accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- null distribution\n",
    "\n",
    "분류자의 일반화 정확도가 우연에 의해 기대되는 것보다 우수한지를 확인하기 위해, 수행을 변화에 의한 어떤 영가설 분포(예: 두 대안적 분류에 대한 이항 분포)에 따라 기대되는 수행과 비교하는 것. 각 분류 집단에 존재하는 관찰치 수가 다르면 정확도에 편향을 가져올 것\n",
    "\n",
    "- resampling approach\n",
    "\n",
    "전체 교차 타당화 절차가 분류집단 라벨이 무선적으로 매번 재할당되면서 여러 번 수행된다. 매번 정확도 값이 저장되고, 이 값들의 분포는 관촬된 수행이 검증될 수 있도록 경험적 영가설 분포를 제공\n",
    "관찰치의 모든 분류 집단들에 대해 따로 정확도를 조사하는 것이 중요\n",
    "\n",
    "- 균형잡힌 정확도 측정 계산의 중요성\n",
    "\n",
    "평균 정확도. Stratified cross-validation scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###9.6 Characterizing the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "분류자 분석을 수행하고, 선택된 특징 세트에서 잘 수행되는지 확인했다면, 분류자가 어떻게 그 수행을 이루어 내는지를 평가하는 것이 중요\n",
    "\n",
    "<b>Sensitivity maps</b>\n",
    "\n",
    "공통된 절차는 분류에 대한 각 복셀의 중요도를 시각화=>sensitivity map(민감도 지도) 또는 중요도 지도(importance map)를 생성. 사용되는 분류자의 종류에 따라 구체적인 속성이 다르지만, 분류자가 수행을 어떻게 했는지 알 수 있음.\n",
    "neural network(신경망): 연결망의 가중치 구조를 다시 뇌에 지도화 하는 것이 가능. 각 복셀의 기요도를 나타내는 중요한 지도 제공\n",
    "SVM: 각 차원에 할당된 매개변수에 기초한 민감도 지도를 계산.</br>\n",
    "\n",
    "잡음을 더하여 특징을 간섭하는 효과를 살펴봄으로써 그 특징들의 중요성을 평가할 수도 있음. 분류자에 따라 영향을 미치는 정도가 다르므로 이를 사용하기 위해서는 분류자에 대한 이해가 필요함\n",
    "\n",
    "<b>Searchlight 과정</b>\n",
    "\n",
    "작은 구가 중앙에 각 복셀을 두면서 놓이고, 분류자는 그 구 내에 있는 복셀들만을 이용하여 훈련되고 시험된다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
